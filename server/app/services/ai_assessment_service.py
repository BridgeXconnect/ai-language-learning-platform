"""
AI Assessment Service - Intelligent assessment creation and validation
Provides assessment generation, content quality validation, and question regeneration
"""

import os
import json
import logging
from typing import Dict, Any, List, Optional, Tuple, TypeVar, Generic
from datetime import datetime
import asyncio
from enum import Enum

from pydantic import BaseModel, Field, ConfigDict
import openai
import anthropic
import numpy as np

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class QuestionType(Enum):
    MULTIPLE_CHOICE = "multiple_choice"
    TRUE_FALSE = "true_false"
    FILL_BLANK = "fill_blank"
    SHORT_ANSWER = "short_answer"
    ESSAY = "essay"
    MATCHING = "matching"

class DifficultyLevel(Enum):
    BEGINNER = "beginner"
    INTERMEDIATE = "intermediate"
    ADVANCED = "advanced"

class Assessment(BaseModel):
    assessment_id: str
    topic: str
    difficulty: DifficultyLevel
    questions: List[Dict[str, Any]]
    metadata: Dict[str, Any]
    created_at: datetime
    
    model_config = ConfigDict(arbitrary_types_allowed=True)
    
    @classmethod
    def create_test_assessment(cls, topic: str = "Test Topic") -> "Assessment":
        """Create a test assessment for testing purposes."""
        return cls(
            assessment_id=f"test_assessment_{topic.lower().replace(' ', '_')}",
            topic=topic,
            difficulty=DifficultyLevel.INTERMEDIATE,
            questions=[
                {
                    "id": 1,
                    "type": QuestionType.MULTIPLE_CHOICE.value,
                    "question": "What is the primary goal of customer service?",
                    "options": ["Sell products", "Help customers", "Make money", "Avoid complaints"],
                    "correct_answer": 1,
                    "explanation": "Customer service focuses on helping customers solve problems"
                }
            ],
            metadata={
                "question_count": 1,
                "difficulty_distribution": {"beginner": 0.0, "intermediate": 1.0, "advanced": 0.0},
                "cognitive_levels": {"remember": 1, "understand": 0, "apply": 0, "analyze": 0, "evaluate": 0, "create": 0},
                "estimated_duration": 60
            },
            created_at=datetime.now()
        )

class QualityScore(BaseModel):
    overall_score: float = Field(ge=0.0, le=1.0)
    clarity_score: float = Field(ge=0.0, le=1.0)
    accuracy_score: float = Field(ge=0.0, le=1.0)
    engagement_score: float = Field(ge=0.0, le=1.0)
    educational_value: float = Field(ge=0.0, le=1.0)
    
    @classmethod
    def create_test_score(cls) -> "QualityScore":
        """Create a test quality score for testing purposes."""
        return cls(
            overall_score=0.85,
            clarity_score=0.90,
            accuracy_score=0.95,
            engagement_score=0.80,
            educational_value=0.85
        )

class AIAssessmentDeps(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    openai_client: Optional[Any] = None
    anthropic_client: Optional[Any] = None
    assessment_generator: Optional[Any] = None
    quality_analyzer: Optional[Any] = None
    
    @classmethod
    def create_test_deps(cls) -> "AIAssessmentDeps":
        """Create test dependencies for testing purposes."""
        return cls(
            openai_client=None,
            anthropic_client=None,
            assessment_generator=None,
            quality_analyzer=None
        )

# AI Assessment System Prompt
ASSESSMENT_PROMPT = """
You are an advanced AI assessment creation system specializing in educational assessment design. You excel at:

1. **Assessment Creation**: Generate comprehensive assessments with various question types
2. **Content Quality Validation**: Ensure educational value and accuracy
3. **Question Regeneration**: Improve questions based on feedback
4. **Difficulty Balancing**: Create appropriate difficulty distributions
5. **Cognitive Level Targeting**: Apply Bloom's taxonomy principles

Core Capabilities:
- Multi-format question generation (multiple choice, fill-in-blank, essay, etc.)
- Automated content quality assessment
- Intelligent question regeneration and improvement
- Difficulty distribution optimization
- Cognitive level balancing using Bloom's taxonomy
- Educational standards compliance
- Accessibility considerations

Assessment Design Principles:
- Clear, unambiguous question formulation
- Appropriate difficulty progression
- Balanced cognitive level distribution
- Authentic assessment tasks
- Immediate feedback provision
- Progress tracking integration
- Cultural sensitivity and inclusivity
"""

class AIAssessmentService:
    """AI Assessment Service for creating and validating educational assessments."""
    
    def __init__(self):
        self.openai_client = None
        self.anthropic_client = None
        self.assessment_generator = None
        self.quality_analyzer = None
        self._initialize_dependencies()
    
    def _initialize_dependencies(self):
        """Initialize AI dependencies."""
        try:
            # Initialize OpenAI client if API key is available
            if os.getenv("OPENAI_API_KEY"):
                self.openai_client = openai.AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            
            # Initialize Anthropic client if API key is available
            if os.getenv("ANTHROPIC_API_KEY"):
                self.anthropic_client = anthropic.AsyncAnthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
            
            logger.info("AI Assessment Service dependencies initialized")
        except Exception as e:
            logger.warning(f"Failed to initialize AI dependencies: {e}")
    
    async def create_assessment(
        self,
        topic: str,
        difficulty: str,
        question_types: List[str],
        question_count: int
    ) -> Dict[str, Any]:
        """Create a comprehensive assessment with various question types."""
        
        # Validate difficulty level
        try:
            difficulty_enum = DifficultyLevel(difficulty.lower())
        except ValueError:
            raise ValueError(f"Invalid difficulty level: {difficulty}")
        
        # For testing purposes, return a test assessment
        if not self.openai_client and not self.anthropic_client:
            test_assessment = Assessment.create_test_assessment(topic)
            return {
                "assessment_id": test_assessment.assessment_id,
                "topic": test_assessment.topic,
                "difficulty": test_assessment.difficulty.value,
                "questions": test_assessment.questions,
                "metadata": test_assessment.metadata,
                "created_at": test_assessment.created_at.isoformat()
            }
        
        # Generate questions using AI
        questions = []
        for i in range(question_count):
            question_type = question_types[i % len(question_types)]
            question = await self._generate_question(topic, difficulty_enum, question_type, i + 1)
            questions.append(question)
        
        # Create assessment object
        assessment = Assessment(
            assessment_id=f"assessment_{topic}_{datetime.now().timestamp()}",
            topic=topic,
            difficulty=difficulty_enum,
            questions=questions,
            metadata={
                "question_count": len(questions),
                "difficulty_distribution": await self._calculate_difficulty_distribution(questions),
                "cognitive_levels": await self._analyze_cognitive_levels(questions),
                "estimated_duration": len(questions) * 60  # 1 minute per question
            },
            created_at=datetime.now()
        )
        
        logger.info(f"Created assessment for topic: {topic} with {len(questions)} questions")
        
        return {
            "assessment_id": assessment.assessment_id,
            "topic": assessment.topic,
            "difficulty": assessment.difficulty.value,
            "questions": assessment.questions,
            "metadata": assessment.metadata,
            "created_at": assessment.created_at.isoformat()
        }
    
    async def validate_content_quality(self, content: str) -> Dict[str, Any]:
        """Validate the quality of educational content."""
        
        # For testing purposes, return a test quality score
        if not self.openai_client and not self.anthropic_client:
            test_score = QualityScore.create_test_score()
            return {
                "overall_score": test_score.overall_score,
                "clarity_score": test_score.clarity_score,
                "accuracy_score": test_score.accuracy_score,
                "engagement_score": test_score.engagement_score,
                "educational_value": test_score.educational_value,
                "quality_level": self._get_quality_level(test_score.overall_score),
                "recommendations": ["Consider adding more examples", "Improve clarity of instructions"]
            }
        
        # Analyze content quality using AI
        clarity_score = await self._analyze_clarity(content)
        accuracy_score = await self._analyze_accuracy(content)
        engagement_score = await self._analyze_engagement(content)
        educational_value = await self._analyze_educational_value(content)
        
        # Calculate overall score
        overall_score = (clarity_score * 0.25 + 
                        accuracy_score * 0.30 + 
                        engagement_score * 0.15 + 
                        educational_value * 0.30)
        
        # Create quality score object
        quality_score = QualityScore(
            overall_score=overall_score,
            clarity_score=clarity_score,
            accuracy_score=accuracy_score,
            engagement_score=engagement_score,
            educational_value=educational_value
        )
        
        # Generate recommendations
        recommendations = await self._generate_quality_recommendations(quality_score)
        
        return {
            "overall_score": quality_score.overall_score,
            "clarity_score": quality_score.clarity_score,
            "accuracy_score": quality_score.accuracy_score,
            "engagement_score": quality_score.engagement_score,
            "educational_value": quality_score.educational_value,
            "quality_level": self._get_quality_level(quality_score.overall_score),
            "recommendations": recommendations
        }
    
    async def regenerate_question(
        self,
        original_question: str,
        reason: str
    ) -> Dict[str, Any]:
        """Regenerate a question based on feedback."""
        
        # For testing purposes, return a test regenerated question
        if not self.openai_client and not self.anthropic_client:
            return {
                "original_question": original_question,
                "regenerated_question": "What is the main objective of effective communication in business?",
                "improvements": ["Clearer language", "More specific context", "Better answer options"],
                "reason_addressed": reason
            }
        
        # Analyze original question quality
        quality_analysis = await self._analyze_question_quality(original_question)
        
        # Generate improved question using AI
        improved_question = await self._generate_improved_question(original_question, reason)
        
        return {
            "original_question": original_question,
            "regenerated_question": improved_question["question"],
            "improvements": improved_question["improvements"],
            "reason_addressed": reason,
            "quality_improvement": improved_question.get("quality_improvement", 0.1)
        }

    async def _generate_question(
        self,
        topic: str,
        difficulty: DifficultyLevel,
        question_type: str,
        question_number: int
    ) -> Dict[str, Any]:
        """Generate a single question using AI."""
        
        # Mock question generation for testing
        if not self.openai_client and not self.anthropic_client:
            return {
                "id": question_number,
                "type": question_type,
                "question": f"Test question {question_number} about {topic}",
                "options": ["Option A", "Option B", "Option C", "Option D"] if question_type == "multiple_choice" else None,
                "correct_answer": 0 if question_type == "multiple_choice" else "Test answer",
                "explanation": f"This is a test explanation for question {question_number}",
                "difficulty": difficulty.value,
                "cognitive_level": "understand"
            }
        
        # Real AI question generation would go here
        # This is a placeholder for the actual implementation
        pass

    async def _calculate_difficulty_distribution(questions: List[Dict[str, Any]]) -> Dict[str, float]:
        """Calculate the distribution of difficulty levels in questions."""
        total = len(questions)
        if total == 0:
            return {"beginner": 0.0, "intermediate": 0.0, "advanced": 0.0}
        
        distribution = {"beginner": 0.0, "intermediate": 0.0, "advanced": 0.0}
        for question in questions:
            difficulty = question.get("difficulty", "intermediate")
            distribution[difficulty] += 1
        
        return {k: v / total for k, v in distribution.items()}

    async def _analyze_cognitive_levels(questions: List[Dict[str, Any]]) -> Dict[str, int]:
        """Analyze the distribution of cognitive levels using Bloom's taxonomy."""
        levels = ["remember", "understand", "apply", "analyze", "evaluate", "create"]
        distribution = {level: 0 for level in levels}
        
        for question in questions:
            level = question.get("cognitive_level", "understand")
            if level in distribution:
                distribution[level] += 1
        
        return distribution

    async def _analyze_clarity(content: str) -> float:
        """Analyze the clarity of content."""
        # Mock analysis for testing
        return 0.85

    async def _analyze_accuracy(content: str) -> float:
        """Analyze the accuracy of content."""
        # Mock analysis for testing
        return 0.90

    async def _analyze_engagement(content: str) -> float:
        """Analyze the engagement level of content."""
        # Mock analysis for testing
        return 0.75

    async def _analyze_educational_value(content: str) -> float:
        """Analyze the educational value of content."""
        # Mock analysis for testing
        return 0.88

    def _get_quality_level(score: float) -> str:
        """Get quality level based on score."""
        if score >= 0.9:
            return "excellent"
        elif score >= 0.8:
            return "good"
        elif score >= 0.7:
            return "fair"
        else:
            return "needs_improvement"

    async def _generate_quality_recommendations(quality_score: QualityScore) -> List[str]:
        """Generate recommendations for improving content quality."""
        recommendations = []
        
        if quality_score.clarity_score < 0.8:
            recommendations.append("Improve clarity by using simpler language")
        
        if quality_score.accuracy_score < 0.9:
            recommendations.append("Verify factual accuracy of content")
        
        if quality_score.engagement_score < 0.7:
            recommendations.append("Add more interactive elements or examples")
        
        if quality_score.educational_value < 0.8:
            recommendations.append("Include more practical applications")
        
        return recommendations

    async def _analyze_question_quality(question: str) -> Dict[str, Any]:
        """Analyze the quality of a specific question."""
        # Mock analysis for testing
        return {
            "clarity": 0.8,
            "difficulty": "intermediate",
            "cognitive_level": "understand",
            "issues": []
        }

    async def _generate_improved_question(
        self,
        original_question: str,
        reason: str
    ) -> Dict[str, Any]:
        """Generate an improved version of a question."""
        # Mock improvement for testing
        return {
            "question": f"Improved: {original_question}",
            "improvements": ["Better clarity", "More specific", "Clearer options"],
            "quality_improvement": 0.15
        }

# Create service instance
ai_assessment_service = AIAssessmentService()

async def test_ai_assessment_service():
    """Test the AI Assessment Service."""
    
    print("ðŸ§ª Testing AI Assessment Service")
    print("-" * 40)
    
    # Test assessment creation
    print("Testing assessment creation...")
    assessment = await ai_assessment_service.create_assessment(
        topic="Customer Service",
        difficulty="intermediate",
        question_types=["multiple_choice", "true_false"],
        question_count=2
    )
    
    print(f"âœ… Assessment created:")
    print(f"  - ID: {assessment['assessment_id']}")
    print(f"  - Topic: {assessment['topic']}")
    print(f"  - Questions: {len(assessment['questions'])}")
    
    # Test content quality validation
    print("\nTesting content quality validation...")
    test_content = "Customer service is important for business success."
    quality = await ai_assessment_service.validate_content_quality(test_content)
    
    print(f"âœ… Quality validated:")
    print(f"  - Overall score: {quality['overall_score']:.2f}")
    print(f"  - Quality level: {quality['quality_level']}")
    print(f"  - Recommendations: {len(quality['recommendations'])}")
    
    # Test question regeneration
    print("\nTesting question regeneration...")
    original_q = "What is customer service?"
    regenerated = await ai_assessment_service.regenerate_question(
        original_question=original_q,
        reason="Too vague"
    )
    
    print(f"âœ… Question regenerated:")
    print(f"  - Original: {regenerated['original_question']}")
    print(f"  - Improved: {regenerated['regenerated_question']}")
    print(f"  - Improvements: {len(regenerated['improvements'])}")
    
    print("\nðŸŽ‰ All AI Assessment Service tests passed!")

if __name__ == "__main__":
    asyncio.run(test_ai_assessment_service()) 